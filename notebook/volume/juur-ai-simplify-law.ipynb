{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## juur.ai - Simplify Law\n",
    "\n",
    "This project is made to simplify the process of understanding Estonian State laws, making it easier for ordinary citizens to navigate complex legal texts. While the Estonian government provides a helper bot called [Bürokratt](https://www.kratid.ee/en/burokratt), it has faced criticism for being unreliable and difficult to use. Our goal is to address these issues by offering a more intuitive, efficient solution that helps users quickly find the legal information they need.\n",
    "\n",
    "Gemini 1.5's large context window, capable of processing up to 2 million tokens, provides the foundation for this project. However, due to the limitations of the free version, which supports only 1 million tokens, we run multiple parallel models to process the information. We also utilize a final model to compare and select the best response, ensuring that users receive the most accurate and relevant legal information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import threading\n",
    "import time\n",
    "import concurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load API keys\n",
    "\n",
    "NB! to use this model at least 6 google API keys are needed.<br>\n",
    "This can either be done by using `.env` file or inputing them into the code.<br>\n",
    "The env file should be formated like\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY_1=abc\n",
    "GOOGLE_API_KEY_2=abc\n",
    "GOOGLE_API_KEY_3=abc\n",
    "GOOGLE_API_KEY_4=abc\n",
    "GOOGLE_API_KEY_5=abc\n",
    "GOOGLE_API_KEY_6=abc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_keys = []\n",
    "if os.path.isfile(\".env\"):\n",
    "    load_dotenv()\n",
    "    api_key_1 = os.getenv('GOOGLE_API_KEY_1')\n",
    "    api_key_2 = os.getenv('GOOGLE_API_KEY_2')\n",
    "    api_key_3 = os.getenv('GOOGLE_API_KEY_3')\n",
    "    api_key_4 = os.getenv('GOOGLE_API_KEY_4')\n",
    "    api_key_5 = os.getenv('GOOGLE_API_KEY_5')\n",
    "    api_key_6 = os.getenv('GOOGLE_API_KEY_6')\n",
    "    \n",
    "    api_keys = [\n",
    "        api_key_1,\n",
    "        api_key_2,\n",
    "        api_key_3,\n",
    "        api_key_4,\n",
    "        api_key_5,\n",
    "        api_key_6\n",
    "    ]\n",
    "else:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_keys = [\n",
    "        user_secrets.get_secret(\"GOOGLE_API_KEY_1\"),\n",
    "        user_secrets.get_secret(\"GOOGLE_API_KEY_2\"),\n",
    "        user_secrets.get_secret(\"GOOGLE_API_KEY_3\"),\n",
    "        user_secrets.get_secret(\"GOOGLE_API_KEY_4\"),\n",
    "        user_secrets.get_secret(\"GOOGLE_API_KEY_5\"),\n",
    "        user_secrets.get_secret(\"GOOGLE_API_KEY_6\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading datasets\n",
    "\n",
    "NB! The dataset is located here: [https://www.kaggle.com/datasets/robinotter/eesti-vabariigi-seadused](http://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making data clusters based on the maximum token count of the LLM\n",
    "\n",
    "We started with 390 legal documents, which we organized into 12 groups. To ensure each document remains intact and fully accessible to a single language model, we sorted them based on their token count. Each group contains up to 998,000 tokens—just under the 1-million-token limit—leaving room for the system and user prompts. If users want to ask longer questions, we may need to reduce the group size to accommodate the extra tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:35:38.555223Z",
     "iopub.status.busy": "2024-12-01T23:35:38.554782Z",
     "iopub.status.idle": "2024-12-01T23:35:38.562863Z",
     "shell.execute_reply": "2024-12-01T23:35:38.561770Z",
     "shell.execute_reply.started": "2024-12-01T23:35:38.555175Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_file_sizes(data_folder, api_key, model_name):\n",
    "    file_sizes = {}\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    for filename in os.listdir(data_folder):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        file_path = os.path.join(data_folder, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            print(f'Processing file: {file_path}')\n",
    "            my_file = open(file_path, 'r')\n",
    "            my_file_content = my_file.read()\n",
    "            my_file.close()\n",
    "            token_count = model.count_tokens(my_file_content).total_tokens\n",
    "            file_sizes[filename] = token_count\n",
    "\n",
    "    return file_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:35:38.564721Z",
     "iopub.status.busy": "2024-12-01T23:35:38.564269Z",
     "iopub.status.idle": "2024-12-01T23:35:38.578226Z",
     "shell.execute_reply": "2024-12-01T23:35:38.577259Z",
     "shell.execute_reply.started": "2024-12-01T23:35:38.564671Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_groups(file_sizes, group_token_limit=1000):\n",
    "    sorted_file_sizes = dict(sorted(file_sizes.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    groups = []\n",
    "\n",
    "    def add_to_group(file_name):\n",
    "        for group in groups:\n",
    "            if sum(group.values()) + file_sizes[file_name] < group_token_limit:\n",
    "                group[file_name] = file_sizes[file_name]\n",
    "                return\n",
    "        groups.append({file_name: file_sizes[file_name]})\n",
    "\n",
    "    for file_name in sorted_file_sizes:\n",
    "        add_to_group(file_name)\n",
    "\n",
    "    return groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:35:38.582251Z",
     "iopub.status.busy": "2024-12-01T23:35:38.581271Z",
     "iopub.status.idle": "2024-12-01T23:35:38.589590Z",
     "shell.execute_reply": "2024-12-01T23:35:38.588337Z",
     "shell.execute_reply.started": "2024-12-01T23:35:38.582188Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_new_files(groups, data_folder, output_folder):\n",
    "    for i, group in enumerate(groups):\n",
    "        i = i + 1\n",
    "        with open(f'{output_folder}/group_{i}.txt', 'w') as f:\n",
    "            for file_name in group:\n",
    "                input_file_path = os.path.join(data_folder, file_name)\n",
    "                with open(input_file_path, 'r') as input_file:\n",
    "                    print(f'Writing {file_name} to group_{i}.txt')\n",
    "                    f.write(input_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/sorted-data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m sorted_data_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/sorted-data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(sorted_data_folder):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_data_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m file_sizes \u001b[38;5;241m=\u001b[39m get_file_sizes(data_folder\u001b[38;5;241m=\u001b[39mdata_folder,api_key\u001b[38;5;241m=\u001b[39mapi_keys[\u001b[38;5;241m0\u001b[39m],model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Note: the token limit should also fit the system prompt and user prompt\u001b[39;00m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/sorted-data'"
     ]
    }
   ],
   "source": [
    "# data_folder = \"/kaggle/input/eesti-vabariigi-seadused/\"\n",
    "data_folder = \"/data\"\n",
    "sorted_data_folder = \"/sorted-data\"\n",
    "\n",
    "if not os.path.exists(sorted_data_folder):\n",
    "    os.makedirs(sorted_data_folder)\n",
    "\n",
    "file_sizes = get_file_sizes(data_folder=data_folder,api_key=api_keys[0],model_name=\"gemini-1.5-flash\")\n",
    "# Note: the token limit should also fit the system prompt and user prompt\n",
    "groups = get_groups(file_sizes=file_sizes, group_token_limit=998000)\n",
    "generate_new_files(groups=groups, data_folder=data_folder, output_folder=sorted_data_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loading sorted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = []\n",
    "sorted_data_folder = \"sorted-data\"\n",
    "\n",
    "for filename in os.listdir(sorted_data_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(sorted_data_folder, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            file_contents.append(content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model configuration and environment setup\n",
    "\n",
    "Our system uses Estonian for both prompts and response formats, as all the legal documents are in Estonian and most of the questions are asked in Estonian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    from: https://stackoverflow.com/questions/3173320/text-progress-bar-in-terminal-with-block-characters\n",
    "    \n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Sa oled seaduste abiline. Kui sulle antud seaduses on küsimusele vastust, siis vasta sellele. Kui vastus puudub, siis vasta '0'\"\n",
    "response_format = \"\"\"kastuta vastamisel järgmist formaati:\n",
    "<seaduse nimi>\n",
    "\n",
    "<sinu vastus>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM threading\n",
    "\n",
    "We utilize threading to generate multiple Gemini responses, as this project requires handling up to 12 million tokens, while the free version of the model is limited to a maximum token count of 1 million. By using multiple models, we can efficiently search for specific paragraphs that explain the law the user has requested. This approach allows us to process large amounts of data, ensuring that relevant and precise information is extracted to answer complex legal queries. The ability to handle such extensive context is crucial for delivering accurate and comprehensive results.\n",
    "\n",
    "\n",
    "Traditionally, finding answers in a 12-million-character legal database could take hours, if not days. Now, with our tool, powered by LLMs, you can get accurate answers in just three minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:37:21.583850Z",
     "iopub.status.busy": "2024-12-01T23:37:21.583460Z",
     "iopub.status.idle": "2024-12-01T23:37:21.612681Z",
     "shell.execute_reply": "2024-12-01T23:37:21.611568Z",
     "shell.execute_reply.started": "2024-12-01T23:37:21.583807Z"
    }
   },
   "outputs": [],
   "source": [
    "thread_local = threading.local()\n",
    "responses_lock = threading.Lock()\n",
    "api_keys_lock = threading.Lock()\n",
    "responses = []\n",
    "\n",
    "errors = []\n",
    "errors_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    if not hasattr(thread_local, \"session\"):\n",
    "        thread_local.session = requests.Session()\n",
    "    return thread_local.session\n",
    "\n",
    "def get_response(data):\n",
    "    api_keys, file_content, group_index, group_count, system_prompt, response_format, question = data\n",
    "    # print(f\"Group {group_index + 1} started\")\n",
    "    \n",
    "    with api_keys_lock:\n",
    "        if not api_keys:\n",
    "            print(\"No API keys left to process.\")\n",
    "            return\n",
    "        api_key = api_keys.pop(0)\n",
    "        \n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    #if group_index % 2 == 0:\n",
    "    #    model = genai.GenerativeModel(\"gemini-1.5-flash-8b\")\n",
    "\n",
    "    try:\n",
    "        response_text = model.generate_content([\n",
    "            system_prompt,\n",
    "            response_format,\n",
    "            file_content,\n",
    "            \"Küsimus on järgmine:\",\n",
    "            question\n",
    "        ]).text\n",
    "    except Exception as e:\n",
    "        with errors_lock:\n",
    "            errors.append((group_index, e))\n",
    "        return\n",
    "\n",
    "    # print(f\"Group {group_index + 1} done\")\n",
    "    # print(response_text)\n",
    "\n",
    "    # Safely append to the shared responses list\n",
    "    with responses_lock:\n",
    "        responses.append((group_index, response_text))\n",
    "        \n",
    "    groups_done = len(responses)\n",
    "    printProgressBar(groups_done, 12, length = 50)\n",
    "\n",
    "def get_all_responses(api_keys, file_contents, system_prompt, response_format, question):\n",
    "    # print(\"Getting responses\")\n",
    "    group_count = len(api_keys)\n",
    "    tasks = [\n",
    "        ( api_keys, file_contents[i], i, group_count, system_prompt, response_format, question)\n",
    "        for i in range(group_count)\n",
    "    ]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        executor.map(get_response, tasks)\n",
    "\n",
    "def control_threading(user_message):\n",
    "    temp_api_keys = api_keys.copy()\n",
    "    # temp_api_keys = temp_api_keys * 2\n",
    "    temp_api_keys.sort()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # group_count = len(api_keys) * 2\n",
    "    printProgressBar(0, 12, length = 50)\n",
    "\n",
    "    get_all_responses(temp_api_keys, file_contents[:6], system_prompt, response_format, user_message)\n",
    "\n",
    "    while start_time + 120 > time.time():\n",
    "        pass\n",
    "        \n",
    "\n",
    "    # print(start_time)\n",
    "    # print(time.time())\n",
    "    temp_api_keys = api_keys.copy()\n",
    "    get_all_responses(temp_api_keys, file_contents[6:], system_prompt, response_format, user_message)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    responses.sort(key=lambda x: x[0])\n",
    "\n",
    "    print()\n",
    "    print(f\"Got responses in {duration} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best response\n",
    "\n",
    "The answers also include citations, specify which legal documents were used in answering, and provide recommendations on where to find additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:37:21.617637Z",
     "iopub.status.busy": "2024-12-01T23:37:21.617165Z",
     "iopub.status.idle": "2024-12-01T23:37:21.630503Z",
     "shell.execute_reply": "2024-12-01T23:37:21.629340Z",
     "shell.execute_reply.started": "2024-12-01T23:37:21.617588Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_best_answer(user_message):\n",
    "    genai.configure(api_key=api_keys[0])\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "    response_texts = [element[1] for element in responses]\n",
    "\n",
    "    print(\"Choosing best answer\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    prompt = (\n",
    "        [\"Sulle antakse mitme erineva mudeli vastused erinevatest seadustest, tsiteeri mulle mudeli vastuseid, mis pole '0'\"] +\n",
    "        [response_format] +\n",
    "        response_texts + \n",
    "        [\"Küsimus on järgnev:\"] +\n",
    "        [user_message]\n",
    "    )\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Chose best answer in {duration} seconds\")\n",
    "    \n",
    "    best_answer = model.generate_content(prompt)\n",
    "    return best_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:37:21.632243Z",
     "iopub.status.busy": "2024-12-01T23:37:21.631914Z",
     "iopub.status.idle": "2024-12-01T23:37:21.645520Z",
     "shell.execute_reply": "2024-12-01T23:37:21.644349Z",
     "shell.execute_reply.started": "2024-12-01T23:37:21.632212Z"
    }
   },
   "outputs": [],
   "source": [
    "def full_flow(user_message):\n",
    "    control_threading(user_message)\n",
    "    return get_best_answer(user_message).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Simple UI\n",
    "\n",
    "Our solution features an interactive chat interface where you can easily communicate with the model. To keep you informed, a progress bar is displayed during each request, as responses can take up to three minutes. The response time is also shown for added transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:38:00.330672Z",
     "iopub.status.busy": "2024-12-01T23:38:00.329179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;35mjuur.ai: Hello! Type 'exit' to end the conversation.\u001b[0m\n",
      " |█████████████████████████████████████████---------| 83.3% \n",
      "Got responses in 161.61473727226257 seconds\n",
      "Choosing best answer\n",
      "Chose best answer in 6.67572021484375e-06 seconds\n",
      "\u001b[0;35mjuur.ai: Meresõiduohutuse seadus\n",
      "\n",
      "§ 45. Veeteel liiklemine\n",
      "4)Väikelaeva või muu veesõiduki juht ei tohi merel või sisevetel liigeldes olla joobeseisundis.\n",
      "4^1)Alkoholijoobes olevaks loetakse väikelaeva või muu veesõiduki juht järgmistel juhtudel:\n",
      "1)väikelaeva või muu veesõiduki juhi ühes grammis veres on vähemalt 0,5 milligrammi alkoholi või tema väljahingatavas õhus on alkoholi 0,25 milligrammi ühe liitri kohta või rohkem;\n",
      "2)jetijuhi ühes grammis veres on vähemalt 0,2 milligrammi alkoholi või tema väljahingatavas õhus on alkoholi 0,1 milligrammi ühe liitri kohta või rohkem.\n",
      "\n",
      "Seega, seaduste järgi ei ole purjuspeaga kaatriga sõitmine lubatud.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "terminal_colors = {\n",
    "    \"purple\": \"\\033[0;35m\",\n",
    "    \"end\": \"\\033[0m\"\n",
    "}\n",
    "\n",
    "\n",
    "def generate_response(user_message):\n",
    "    responses = []\n",
    "    answer = full_flow(user_message)\n",
    "\n",
    "    return answer\n",
    "\n",
    "print(f\"{terminal_colors['purple']}juur.ai: Hello! Type 'exit' to end the conversation.{terminal_colors['end']}\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    print()\n",
    "    response = generate_response(user_input)\n",
    "    print(f\"{terminal_colors['purple']}juur.ai: {response}{terminal_colors['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Kasuliku mudeli seadus\\n\\n0\\n'),\n",
       " (1, 'Meresõiduohutuse seadus\\n\\n0\\n'),\n",
       " (2,\n",
       "  'Täitemenetluse seadustik\\n\\n0\\n\\nAutoriõiguse seadus\\n\\n0\\n\\nEhitusseadustik\\n\\n0\\n\\nAudiitortegevuse seadus\\n\\n0\\n\\nFinantsinspektsiooni seadus\\n\\n0\\n\\nEuroopa Liidu ühise põllumajanduspoliitika rakendamise seadus\\n\\n0\\n\\nVangistusseadus\\n\\n0\\n\\nMaapõueseadus\\n\\n0\\n\\nVälja kuulutanud\\n\\n0\\n'),\n",
       " (3, 'Liiklusseadus\\n\\n0\\n'),\n",
       " (4, 'Autoveoseadus\\n\\n0\\n')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9881586,
     "sourceId": 83735,
     "sourceType": "competition"
    },
    {
     "datasetId": 6147758,
     "sourceId": 10070495,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
